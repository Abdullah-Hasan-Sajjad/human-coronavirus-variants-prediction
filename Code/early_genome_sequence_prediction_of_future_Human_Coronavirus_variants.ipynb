{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a0c971",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-03-17T12:31:43.596619Z",
          "iopub.status.busy": "2024-03-17T12:31:43.596280Z",
          "iopub.status.idle": "2024-03-17T12:31:56.043253Z",
          "shell.execute_reply": "2024-03-17T12:31:56.042454Z"
        },
        "papermill": {
          "duration": 12.45393,
          "end_time": "2024-03-17T12:31:56.045477",
          "exception": false,
          "start_time": "2024-03-17T12:31:43.591547",
          "status": "completed"
        },
        "tags": [],
        "id": "c4a0c971",
        "outputId": "5a3cfe0f-09e7-4b0c-97cc-b55e0ec9acb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0306e735",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-17T12:31:56.052891Z",
          "iopub.status.busy": "2024-03-17T12:31:56.052325Z",
          "iopub.status.idle": "2024-03-17T12:31:56.056756Z",
          "shell.execute_reply": "2024-03-17T12:31:56.055924Z"
        },
        "papermill": {
          "duration": 0.00999,
          "end_time": "2024-03-17T12:31:56.058598",
          "exception": false,
          "start_time": "2024-03-17T12:31:56.048608",
          "status": "completed"
        },
        "tags": [],
        "id": "0306e735"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "latent_dim = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42057011",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-17T12:31:56.065117Z",
          "iopub.status.busy": "2024-03-17T12:31:56.064839Z",
          "iopub.status.idle": "2024-03-17T12:31:56.070364Z",
          "shell.execute_reply": "2024-03-17T12:31:56.069541Z"
        },
        "papermill": {
          "duration": 0.01092,
          "end_time": "2024-03-17T12:31:56.072306",
          "exception": false,
          "start_time": "2024-03-17T12:31:56.061386",
          "status": "completed"
        },
        "tags": [],
        "id": "42057011"
      },
      "outputs": [],
      "source": [
        "def prepare_data(timeseries_data, n_rows):\n",
        "    X, y =[],[]\n",
        "    for i in range(len(timeseries_data)):\n",
        "        end_ix = i + n_rows\n",
        "        if end_ix > len(timeseries_data)-1:\n",
        "            break\n",
        "        seq_x, seq_y = ''.join(timeseries_data[i:end_ix]), timeseries_data[end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d942dab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-17T12:31:56.079631Z",
          "iopub.status.busy": "2024-03-17T12:31:56.079347Z",
          "iopub.status.idle": "2024-03-17T13:33:14.954052Z",
          "shell.execute_reply": "2024-03-17T13:33:14.953110Z"
        },
        "papermill": {
          "duration": 3678.880955,
          "end_time": "2024-03-17T13:33:14.956186",
          "exception": false,
          "start_time": "2024-03-17T12:31:56.075231",
          "status": "completed"
        },
        "tags": [],
        "id": "1d942dab"
      },
      "outputs": [],
      "source": [
        "best_accuracies = {}\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/kaggle/input/dummy-genome-for-work/Corona Virus Genome Sequences time sorted_v1.csv\")\n",
        "\n",
        "# remove \\n and \\r\n",
        "df['GenomeSequence'] = df['GenomeSequence'].apply(lambda x: ''.join(x.split('\\n')[1:]))\n",
        "df['genome_sequence'] = df['GenomeSequence']\n",
        "df['genome_sequence'] = df['genome_sequence'].str.replace('\\r', '')\n",
        "df['genome_sequence'] = df['genome_sequence'].str.replace('\\n', '')\n",
        "#df['genomes'] = df['genomes'].str.replace('-', '')\n",
        "df['genome_sequence'] = df['genome_sequence'].str.replace('K', '')\n",
        "# add . after each GenomeSequence\n",
        "#df['GenomeSequence'] = df['GenomeSequence'] + '.'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "df = pd.DataFrame({'genome_sequence': [\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG',\n",
        "    'AATCGTATCGCATCG'\n",
        "]})\n",
        "\"\"\"\n",
        "\n",
        "sequence_cut_length = 300\n",
        "#ending_sequence = df['genome_sequence'].str.len().max()\n",
        "#print(ending_sequence)\n",
        "ending_sequence =  27299\n",
        "starting_sequence = 0\n",
        "\n",
        "for sequenceCutStart in range(starting_sequence, ending_sequence, sequence_cut_length):\n",
        "\n",
        "    df['GenomeSequenceSliced'] = df['genome_sequence'].apply(lambda x: x[sequenceCutStart : sequenceCutStart+sequence_cut_length])\n",
        "    #print(df['GenomeSequenceSliced'])\n",
        "    print(f\"Now working on sequence {sequenceCutStart} to {sequenceCutStart+sequence_cut_length-1}\")\n",
        "\n",
        "\n",
        "    trainEnd = 8\n",
        "    validationEnd = 9\n",
        "    n_steps = 1\n",
        "\n",
        "    train_genome, mutated_genome_train = prepare_data(df['GenomeSequenceSliced'].tolist()[:trainEnd], n_steps)\n",
        "    validation_genome, mutated_genome_validation = prepare_data(df['GenomeSequenceSliced'].tolist()[trainEnd-1:validationEnd], n_steps)\n",
        "\n",
        "    mutated_genome_train = [\"\\t\" + genome + \"\\n\" for genome in mutated_genome_train]\n",
        "    mutated_genome_validation = [\"\\t\" + genome + \"\\n\" for genome in mutated_genome_validation]\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"Training genomes : \")\n",
        "    print(f\"{train_genome} \\n\\n\")\n",
        "\n",
        "    print(\"Mutated genomes Train : \")\n",
        "    print(f\"{mutated_genome_train} \\n\\n\")\n",
        "\n",
        "    print(\"Validation genomes : \")\n",
        "    print(f\"{validation_genome} \\n\\n\")\n",
        "\n",
        "    print(\"Mutated genomes Validation : \")\n",
        "    print(f\"{mutated_genome_validation} \\n\\n\")\n",
        "\n",
        "    print(\"Test genomes : \")\n",
        "    print(f\"{test_genome} \\n\\n\")\n",
        "\n",
        "    print(\"Mutated genomes Test : \")\n",
        "    print(f\"{mutated_genome_test} \\n\\n\")\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorize the data.\n",
        "    # Train\n",
        "\n",
        "    input_texts = train_genome\n",
        "    target_texts = mutated_genome_train\n",
        "    input_characters = set()\n",
        "    target_characters = set()\n",
        "\n",
        "    for input_text in input_texts:\n",
        "        for char in input_text:\n",
        "            if char not in input_characters:\n",
        "                input_characters.add(char)\n",
        "\n",
        "    for target_text in target_texts:\n",
        "        for char in target_text:\n",
        "            if char not in target_characters:\n",
        "                target_characters.add(char)\n",
        "\n",
        "    input_characters.add(\" \")\n",
        "    target_characters.add(\" \")\n",
        "\n",
        "    input_characters = sorted(list(input_characters))\n",
        "    target_characters = sorted(list(target_characters))\n",
        "    num_encoder_tokens = len(input_characters)\n",
        "    num_decoder_tokens = len(target_characters)\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "    \"\"\"\n",
        "    print(\"Number of samples:\", len(input_texts))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "    \"\"\"\n",
        "    input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "    target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "\n",
        "    for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "        for t, char in enumerate(input_text):\n",
        "            encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "        encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "        decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "        decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "\n",
        "    # Vectorize the data.\n",
        "    # Validation\n",
        "\n",
        "    input_texts_validation = validation_genome\n",
        "    target_texts_validation = mutated_genome_validation\n",
        "    input_characters_validation = set()\n",
        "    target_characters_validation = set()\n",
        "\n",
        "    for input_text_validation in input_texts_validation:\n",
        "        for char in input_text_validation:\n",
        "            if char not in input_characters_validation:\n",
        "                input_characters_validation.add(char)\n",
        "\n",
        "    for target_text_validation in target_texts_validation:\n",
        "        for char in target_text_validation:\n",
        "            if char not in target_characters_validation:\n",
        "                target_characters_validation.add(char)\n",
        "\n",
        "    input_characters_validation.add(\" \")\n",
        "    target_characters_validation.add(\" \")\n",
        "\n",
        "    input_characters_validation = sorted(list(input_characters_validation))\n",
        "    target_characters_validation = sorted(list(target_characters_validation))\n",
        "    num_encoder_tokens_validation = len(input_characters_validation)\n",
        "    num_decoder_tokens_validation = len(target_characters_validation)\n",
        "    max_encoder_seq_length_validation = max([len(txt) for txt in input_texts_validation])\n",
        "    max_decoder_seq_length_validation = max([len(txt) for txt in target_texts_validation])\n",
        "    \"\"\"\n",
        "    print(\"Number of samples:\", len(input_texts_validation))\n",
        "    print(\"Number of unique input tokens:\", num_encoder_tokens_validation)\n",
        "    print(\"Number of unique output tokens:\", num_decoder_tokens_validation)\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length_validation)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length_validation)\n",
        "    \"\"\"\n",
        "    input_token_index_validation = dict([(char, i) for i, char in enumerate(input_characters_validation)])\n",
        "    target_token_index_validation = dict([(char, i) for i, char in enumerate(target_characters_validation)])\n",
        "\n",
        "    encoder_input_data_validation = np.zeros(\n",
        "        (len(input_texts_validation), max_encoder_seq_length_validation, num_encoder_tokens_validation),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_input_data_validation = np.zeros(\n",
        "        (len(input_texts_validation), max_decoder_seq_length_validation, num_decoder_tokens_validation),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "    decoder_target_data_validation = np.zeros(\n",
        "        (len(input_texts_validation), max_decoder_seq_length_validation, num_decoder_tokens_validation),\n",
        "        dtype=\"float32\",\n",
        "    )\n",
        "\n",
        "    for i, (input_text_validation, target_text_validation) in enumerate(zip(input_texts_validation, target_texts_validation)):\n",
        "        for t, char in enumerate(input_text_validation):\n",
        "            encoder_input_data_validation[i, t, input_token_index_validation[char]] = 1.0\n",
        "        encoder_input_data_validation[i, t + 1 :, input_token_index_validation[\" \"]] = 1.0\n",
        "        for t, char in enumerate(target_text_validation):\n",
        "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "            decoder_input_data_validation[i, t, target_token_index_validation[char]] = 1.0\n",
        "            if t > 0:\n",
        "                # decoder_target_data will be ahead by one timestep\n",
        "                # and will not include the start character.\n",
        "                decoder_target_data_validation[i, t - 1, target_token_index_validation[char]] = 1.0\n",
        "        decoder_input_data_validation[i, t + 1 :, target_token_index_validation[\" \"]] = 1.0\n",
        "        decoder_target_data_validation[i, t:, target_token_index_validation[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "    # Build the model\n",
        "\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "        encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "        decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "        decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "        decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "        model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        checkpoint_filepath = '/kaggle/working/s2s_model.h5'\n",
        "        model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "            filepath = checkpoint_filepath,\n",
        "            monitor = 'val_accuracy',\n",
        "            mode = 'max',\n",
        "            save_freq = \"epoch\",\n",
        "            save_best_only = True,\n",
        "            verbose=0)\n",
        "\n",
        "        #model.save(\"s2s_model.keras\")\n",
        "\n",
        "        #plot_path = 'model_plot.png'\n",
        "        # Save the model plot to a file\n",
        "        #plot_model(model, to_file=plot_path, show_shapes=True)\n",
        "        #model.summary()\n",
        "\n",
        "        history = model.fit(\n",
        "            [encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data = [[encoder_input_data_validation, decoder_input_data_validation], decoder_target_data_validation],\n",
        "            callbacks = [model_checkpoint_callback]\n",
        "        )\n",
        "\n",
        "        # Find the epoch with the highest validation accuracy\n",
        "        best_epoch = history.history['val_accuracy'].index(max(history.history['val_accuracy'])) + 1\n",
        "        best_accuracy = max(history.history['val_accuracy'])\n",
        "\n",
        "        print(f\"\\n Best accuracy: {best_accuracy} at epoch {best_epoch}\")\n",
        "        # store the accuracy for this model\n",
        "        model_name = f'sequence_{sequenceCutStart}_to_{sequenceCutStart+sequence_cut_length-1}'\n",
        "        best_accuracies[model_name] = best_accuracy\n",
        "\n",
        "        # Plot training & validation accuracy values\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        title_text = f'Model accuracy for sequence {sequenceCutStart} to {sequenceCutStart+sequence_cut_length-1} \\n'\n",
        "        title_text += f'Best accuracy: {best_accuracy} at epoch {best_epoch}'\n",
        "        plt.title(title_text)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        accuracy_plot_filename = f'accuracy_plot_{sequenceCutStart}_to_{sequenceCutStart+sequence_cut_length-1}.png'\n",
        "        plt.savefig(accuracy_plot_filename)  # Save the plot as an image\n",
        "        plt.show()\n",
        "\n",
        "        # Plot training & validation loss values\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title(f'Model loss for sequence {sequenceCutStart} to {sequenceCutStart+sequence_cut_length-1}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "        loss_plot_filename = f'loss_plot_{sequenceCutStart}_to_{sequenceCutStart+sequence_cut_length-1}.png'\n",
        "        plt.savefig(loss_plot_filename)  # Save the plot as an image\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72531f20",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-17T13:33:21.919875Z",
          "iopub.status.busy": "2024-03-17T13:33:21.919232Z",
          "iopub.status.idle": "2024-03-17T13:33:21.925987Z",
          "shell.execute_reply": "2024-03-17T13:33:21.925064Z"
        },
        "papermill": {
          "duration": 3.453534,
          "end_time": "2024-03-17T13:33:21.927825",
          "exception": false,
          "start_time": "2024-03-17T13:33:18.474291",
          "status": "completed"
        },
        "tags": [],
        "id": "72531f20"
      },
      "outputs": [],
      "source": [
        "# save the best validation accuracies dictionary in the json file\n",
        "\n",
        "file_path = f'best_validation_accuracies_for_sequence_{starting_sequence}_to_{sequenceCutStart+sequence_cut_length-1}.txt'\n",
        "\n",
        "# Save the dictionary to a text file\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(best_accuracies, file)\n",
        "\n",
        "# print the average of the best validation accuracies\n",
        "# Calculate the average accuracy\n",
        "average_accuracy = sum(best_accuracies.values()) / len(best_accuracies)\n",
        "\n",
        "print(f\"The average accuracy for sequence {starting_sequence} to {sequenceCutStart+sequence_cut_length-1} is: {average_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4244461,
          "sourceId": 7314317,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4108021,
          "sourceId": 7350329,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30626,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3708.318572,
      "end_time": "2024-03-17T13:33:28.482780",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-03-17T12:31:40.164208",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}